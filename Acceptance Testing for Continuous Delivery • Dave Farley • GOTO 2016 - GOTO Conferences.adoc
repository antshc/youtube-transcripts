= Acceptance Testing for Continuous Delivery
Dave Farley • GOTO 2016
:doctype: presentation
:toc: left
:toclevels: 2

= Acceptance Testing Guidelines
Dave Farley • GOTO 2016
:toc: left
:toclevels: 2

== Anti-Patterns in Acceptance Testing

*Don’t* use **UI Record-and-Playback Systems**  
*Don’t* Record-and-Playback production data. This has a role, but it is **NOT** Acceptance Testing.  
*Don’t* dump production data into your test systems — instead, define the **absolute minimum data** required.  
*Don’t* assume **Nasty Automated Testing Products™** will do what you need. Be sceptical.  
Start with **your strategy** and evaluate tools against it.  
*Don’t* have a **separate Testing/QA team** — quality is everyone’s responsibility.  
Developers own Acceptance Tests!  
*Don’t* let every test start and initialize the app.  
Optimise for **Cycle-Time** — be efficient in using test environments.  
*Don’t* include systems **outside your control** in your Acceptance Test scope.  
*Don’t* put `wait()` instructions in tests hoping it will solve intermittency.

---

== Tricks for Success

*Do* ensure that **developers own the tests**.  
*Do* focus your tests on **“What” not “How”**.  
*Do* think of your tests as **Executable Specifications**.  
*Do* make acceptance testing part of your **Definition of Done**.  
*Do* keep tests **isolated from one another**.  
*Do* keep your tests **repeatable**.  
*Do* use the **language of the problem domain** — adopt the DSL approach regardless of technology.  
*Do* **stub external systems**.  
*Do* test in **production-like environments**.  
*Do* make instructions appear **synchronous at the test-case level**.  
*Do* test for **any change**.  
*Do* keep your tests **efficient**.

== Test Environment Types

* Some tests need special treatment.  
* Tag tests with properties and allocate them dynamically.

=== Example Test Annotations

[source,java]
----
@TimeTravel
@Test
public void shouldDoSomethingThatNeedsFakeTime() {
    ...
}

@Destructive
@Test
public void shouldDoSomethingThatKillsPartOfTheSystem() {
    ...
}

@FPGA(version = 1.3)
@Test
public void shouldDoSomethingThatRequiresSpecificHardware() {
    ...
}
----

=== Key Concepts
* Identify tests that require **special environments** (e.g., time manipulation, destructive actions, hardware dependencies).
* Use **metadata or annotations** to dynamically allocate tests to proper environments.
* Manage specialized test resources automatically to ensure scalability and maintainability.
* Keep tests **self-describing** — environment needs should be explicit in metadata.

---

== Test Isolation

* Any form of testing is about **evaluating something in controlled circumstances**.  
* Isolation ensures predictability, reproducibility, and reliability.

=== Isolation Works on Multiple Levels
* **Isolating the System Under Test (SUT)** — test only what is within your responsibility.
* **Isolating test cases from each other** — enable parallel execution without resource conflicts.
* **Isolating test cases from themselves (temporal isolation)** — repeatable tests must not depend on prior state.

=== Strategic Insight
* **Isolation is a vital part of your test strategy.**
* Poor isolation leads to flaky tests, non-deterministic results, and unreliable feedback loops.
* Design environments, data, and infrastructure to support complete isolation of test executions.

== Thesis Summary and Strategic Insights

=== 1. Acceptance Tests Are *Executable Specifications*
Acceptance tests are not mere validations — they are *executable specifications* of system behaviour.  
They define, in machine-verifiable form, what “done” means from a **user’s perspective**, not a developer’s.

> “A good acceptance test is an executable specification for the behaviour of the system.”

==== Implications for Teams
* Treat tests as *contracts* between business and engineering.
* Automate them early and maintain them as core artefacts.
* Use domain-specific or business-readable language (DSLs, Gherkin, SpecFlow).

==== Extended Idea
In modern DevOps environments, executable specifications should also feed *live documentation* — API behaviour docs, compliance verification, and operational readiness dashboards.

---

=== 2. Continuous Delivery Is About *Fast Feedback Loops*
Farley frames development as a hierarchy of **feedback loops**:
* *Inner loop:* TDD → fast developer confidence (minutes)
* *Middle loop:* Acceptance testing → system-level confidence (hours)
* *Outer loop:* Continuous delivery → customer feedback (days/weeks)

The faster these loops operate, the faster and safer the organization can deliver.

==== Implications
* Optimize acceptance tests for feedback in **under one hour**.
* Continuously measure *time from commit to confidence*.
* Treat slow feedback as a *process defect*.

==== Extended Idea
Expose feedback loop metrics in CI/CD dashboards — include test duration, stability, and failure root-cause ratios.

---

=== 3. Developers Own Acceptance Tests
Farley strongly rejects the separation of QA automation and development.

> “Developers are the people who make changes that break tests; therefore, they must be the people responsible for making them pass.”

==== Implications
* Merge QA automation into engineering responsibility.
* Include acceptance test success in the *Definition of Done*.
* Involve QA early as *spec authors* and *test designers*, not downstream executors.

==== Extended Idea
Shift-left testing: collaborate during backlog grooming to write executable acceptance criteria *before* coding.

---

=== 4. Test Design: Focus on *What*, Not *How*
Anti-pattern: tests tightly coupled to implementation details (e.g., UI recorders, brittle APIs).

==== Thesis
Tests should express *intent* (“what”), not *mechanics* (“how”).

==== Implications
* Abstract communication channels (test “drivers” or adapters).
* Avoid UI-based automation; focus on domain-level behaviours.
* Fix interface changes in one place — not across all test cases.

==== Extended Idea
Treat test layers like clean architecture:
Acceptance tests depend on *business intent*, not *interface mechanics*.

---

=== 5. Isolation and Repeatability Are Non-Negotiable
> “Each test must be isolated from others, and rerunning it should yield identical results.”

==== Key Techniques
* **Functional aliasing:** dynamically generate unique entities (users, IDs, etc.) per test run.
* **Controlled state:** avoid shared environments or test data.
* **Parallel execution:** enable concurrency safely.

==== Extended Idea
Use *ephemeral environments* — TestContainers, Kubernetes namespaces, or Terraform workspaces — for full test isolation.

---

=== 6. Acceptance Tests Drive System Design and Deployment Maturity
Acceptance tests act as *deployment rehearsals*.

> “By the time a release candidate reaches production, deployment should be a non-event.”

==== Implications
* Run acceptance tests in **production-like environments**.
* Automate deployments, configuration, and infrastructure validation.
* Treat acceptance tests as *deployment rehearsals* and *compliance gates*.

==== Extended Idea
Integrate acceptance tests with Infrastructure-as-Code pipelines.
Automate validation via *canary rollouts*, *smoke tests*, or *synthetic transactions*.

---

=== 7. Interface Testing and Decoupling Strategies
Full end-to-end tests across multiple systems create coupling and slow feedback.

> “Full end-to-end tests across multiple systems are anti-patterns when they prevent precise control of state.”

==== Strategy
* Each team tests its *own system boundaries*.
* Use *contract testing* to verify interfaces.
* Exchange interface contracts across teams via CI/CD pipelines.

==== Extended Idea
Adopt *consumer-driven contract testing* (e.g., Pact, Hoverfly, WireMock).
Teams validate dependencies autonomously while preserving integration confidence.

---

=== 8. The Role of Domain Language and DSLs
> “We use the language of the problem domain to express our needs in automated testing.”

==== Implications
* Build domain-specific languages (DSLs) to make tests readable and maintainable.
* Ensure both business and developers understand test intent.
* Keep test logic at the domain level, not technical API level.

==== Extended Idea
Combine DSLs with *model-based* and *AI-generated tests* to discover untested behaviour paths automatically.

---

=== 9. Efficiency as a Cultural Principle
A test suite that takes days to run indicates structural inefficiency.

> “Feedback under an hour is a game-changing level of feedback.”

==== Implications
* Optimize for execution time, parallelism, and targeted testing.
* Treat test performance as seriously as production performance.
* Continuously profile and tune test pipelines.

==== Extended Idea
Adopt *observability-driven testing*: measure test reliability, flakiness rate, and runtime as primary CI/CD metrics.

---

=== 10. Continuous Delivery Is the Sum of Disciplined Feedback Loops
Acceptance testing for CD is not just verification — it’s **designing for change**.

==== Core Synthesis
* *Executable specifications* → shared understanding  
* *Ownership* → closed feedback loops  
* *Isolation* → reliable automation  
* *Fast feedback* → faster innovation

==== Strategic Message for Management
Continuous Delivery succeeds when acceptance testing becomes an *engineering discipline*, not a QA phase.  
Leading organizations (e.g., LMAX) treat acceptance testing as part of *system design*, *deployment verification*, and *organizational learning*.

---

